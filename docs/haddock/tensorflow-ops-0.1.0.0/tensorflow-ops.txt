-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | Friendly layer around TensorFlow bindings.
--   
--   Please see README.md
@package tensorflow-ops
@version 0.1.0.0


-- | This module contains definitions for some built-in TensorFlow
--   operations.
--   
--   Note that certain, "stateful" ops like <tt>variable</tt> and
--   <tt>assign</tt> return a <a>Build</a> action (e.g., <tt>Build (Tensor
--   Ref a)</tt> instead of a pure value; the returned <a>Tensor</a>s are
--   always rendered in the current <a>Build</a> context. This approach
--   helps us avoid problems with inlining or common subexpression
--   elimination, by writing
--   
--   <pre>
--   do
--       v &lt;- variable []
--       w &lt;- assign v 3
--       render $ w * w
--   </pre>
--   
--   instead of
--   
--   <pre>
--   let
--      v = variable []
--      w = assign v 3
--   in w * w
--   </pre>
--   
--   since the latter could be reasonably transformed by the compiler into
--   (or vice versa)
--   
--   <pre>
--   let
--      v = variable []
--      w = assign v 3
--      w' = assign v 3
--   in w * w'
--   </pre>
--   
--   Ops should return a <a>Build</a> action if their original
--   <tt>OpDef</tt> marks them as stateful, or if they take any Refs as
--   input. (This mirrors the rules that TensorFlow uses to avoid common
--   subexpression elimination.)
module TensorFlow.Ops

-- | Returns x + y element-wise.
--   
--   <ul>
--   <li>NOTE*: <tt>Add</tt> supports broadcasting. <tt>AddN</tt> does not.
--   More about broadcasting <a>here</a></li>
--   </ul>
add :: (TensorType t, OneOf ((:) * (Complex Double) ((:) * (Complex Float) ((:) * ByteString ((:) * Int16 ((:) * Int32 ((:) * Int64 ((:) * Int8 ((:) * Word16 ((:) * Word8 ((:) * Double ((:) * Float ([] *)))))))))))) t) => Tensor v1 t -> Tensor v2 t -> Tensor Value t

-- | Computes the absolute value of a tensor.
--   
--   Given a tensor <tt>x</tt>, this operation returns a tensor containing
--   the absolute value of each element in <tt>x</tt>. For example, if x is
--   an input element and y is an output element, this operation computes
--   \(y = |x|\).
abs :: (TensorType t, OneOf ((:) * Int32 ((:) * Int64 ((:) * Word16 ((:) * Double ((:) * Float ([] *)))))) t) => Tensor v1 t -> Tensor Value t

-- | Add all input tensors element wise.
addN :: (TensorType t, OneOf ((:) * (Complex Double) ((:) * (Complex Float) ((:) * Int16 ((:) * Int32 ((:) * Int64 ((:) * Int8 ((:) * Word16 ((:) * Word8 ((:) * Double ((:) * Float ([] *))))))))))) t) => [Tensor v1 t] -> Tensor Value t

-- | Returns the index with the largest value across dimensions of a
--   tensor.
argMax :: (TensorType t, OneOf ((:) * (Complex Double) ((:) * (Complex Float) ((:) * Int16 ((:) * Int32 ((:) * Int64 ((:) * Int8 ((:) * Word16 ((:) * Word8 ((:) * Double ((:) * Float ([] *))))))))))) t, TensorType tidx, OneOf ((:) * Int32 ((:) * Int64 ([] *))) tidx) => Tensor v1 t -> Tensor v2 tidx -> Tensor Value Int64

-- | Update <tt>ref</tt> by assigning <a>value</a> to it.
--   
--   This operation outputs "ref" after the assignment is done. This makes
--   it easier to chain operations that need to use the reset value.
assign :: TensorType t => Tensor Ref t -> Tensor v2 t -> Build (Tensor Ref t)

-- | Return the reduction indices for computing gradients of s0 op s1 with
--   broadcast.
--   
--   This is typically used by gradient computations for a broadcasting
--   operation.
broadcastGradientArgs :: (TensorType t, OneOf ((:) * Int32 ((:) * Int64 ([] *))) t) => Tensor v1 t -> Tensor v2 t -> (Tensor Value t, Tensor Value t)

-- | Cast x of type SrcT to y of DstT.
cast :: (TensorType srcT, TensorType dstT) => Tensor v1 srcT -> Tensor Value dstT

-- | Concatenates tensors along one dimension.
concat :: TensorType t => Tensor v1 Int32 -> [Tensor v2 t] -> Tensor Value t

-- | Create a constant tensor.
--   
--   The values should be in row major order, e.g.,
--   
--   element 0: index (0, ..., 0) element 1: index (0, ..., 1) ...
constant :: TensorType a => Shape -> [a] -> Tensor Value a

-- | Returns the truth value of (x == y) element-wise.
--   
--   <ul>
--   <li>NOTE*: <tt>Equal</tt> supports broadcasting. More about
--   broadcasting <a>here</a></li>
--   </ul>
equal :: (TensorType t, OneOf ((:) * (Complex Double) ((:) * (Complex Float) ((:) * Bool ((:) * ByteString ((:) * Int16 ((:) * Int32 ((:) * Int64 ((:) * Int8 ((:) * Word16 ((:) * Word8 ((:) * Double ((:) * Float ([] *))))))))))))) t) => Tensor v1 t -> Tensor v2 t -> Tensor Value Bool
expandDims :: (TensorType t) => Tensor v1 t -> Tensor v2 Int32 -> Tensor Value t

-- | Creates a variable initialized to the given value. Initialization
--   happens next time session runs.
initializedVariable :: TensorType a => Tensor Value a -> Build (Tensor Ref a)

-- | Creates a zero-initialized variable with the given shape.
zeroInitializedVariable :: (TensorType a, Num a) => Shape -> Build (Tensor Ref a)

-- | Creates a tensor filled with a scalar value.
--   
--   This operation creates a tensor of shape <tt>dims</tt> and fills it
--   with <a>value</a>.
--   
--   For example:
--   
--   ```prettyprint # Output tensor has shape [2, 3]. fill([2, 3], 9)
--   ==&gt; [[9, 9, 9] [9, 9, 9]] ```
fill :: TensorType t => Tensor v1 Int32 -> Tensor v2 t -> Tensor Value t

-- | Returns a one-hot tensor.
--   
--   The locations represented by indices in <tt>indices</tt> take value
--   <tt>on_value</tt>, while all other locations take value
--   <tt>off_value</tt>.
--   
--   If the input <tt>indices</tt> is rank <tt>N</tt>, the output will have
--   rank `N+1`, The new axis is created at dimension <tt>axis</tt>
--   (default: the new axis is appended at the end).
--   
--   If <tt>indices</tt> is a scalar the output shape will be a vector of
--   length <tt>depth</tt>.
--   
--   If <tt>indices</tt> is a vector of length <tt>features</tt>, the
--   output shape will be: ``` features x depth if axis == -1 depth x
--   features if axis == 0 ```
--   
--   If <tt>indices</tt> is a matrix (batch) with shape `[batch,
--   features]`, the output shape will be: ``` batch x features x depth if
--   axis == -1 batch x depth x features if axis == 1 depth x batch x
--   features if axis == 0 ```
--   
--   Examples =========
--   
--   Suppose that
--   
--   ``` indices = [0, 2, -1, 1] depth = 3 on_value = 5.0 off_value = 0.0
--   axis = -1 ```
--   
--   Then output is `[4 x 3]`:
--   
--   ```output = [5.0 0.0 0.0] // one_hot(0) [0.0 0.0 5.0] // one_hot(2)
--   [0.0 0.0 0.0] // one_hot(-1) [0.0 5.0 0.0] // one_hot(1) ```
--   
--   Suppose that
--   
--   ``` indices = [0, 2, -1, 1] depth = 3 on_value = 0.0 off_value = 3.0
--   axis = 0 ```
--   
--   Then output is `[3 x 4]`:
--   
--   ```output = [0.0 3.0 3.0 3.0] [3.0 3.0 3.0 0.0] [3.0 3.0 3.0 3.0] [3.0
--   0.0 3.0 3.0] // ^ one_hot(0) // ^ one_hot(2) // ^ one_hot(-1) // ^
--   one_hot(1) ``` Suppose that
--   
--   ``` indices = [[0, 2], [1, -1]] depth = 3 on_value = 1.0 off_value =
--   0.0 axis = -1 ```
--   
--   Then output is `[2 x 2 x 3]`:
--   
--   ```output = [ [1.0, 0.0, 0.0] // one_hot(0) [0.0, 0.0, 1.0] //
--   one_hot(2) ][ [0.0, 1.0, 0.0] // one_hot(1) [0.0, 0.0, 0.0] //
--   one_hot(-1) ]```
oneHot :: (TensorType t, TensorType tI, OneOf ((:) * Int32 ((:) * Int64 ((:) * Word8 ([] *)))) tI) => Tensor v1 tI -> Tensor v2 Int32 -> Tensor v3 t -> Tensor v4 t -> Tensor Value t

-- | Multiply the matrix "a" by the matrix "b".
--   
--   The inputs must be two-dimensional matrices and the inner dimension of
--   "a" (after being transposed if transpose_a is true) must match the
--   outer dimension of "b" (after being transposed if transposed_b is
--   true).
--   
--   <ul>
--   <li>Note*: The default kernel implementation for MatMul on GPUs uses
--   cublas.</li>
--   </ul>
matMul :: (TensorType t, OneOf ((:) * (Complex Double) ((:) * (Complex Float) ((:) * Int32 ((:) * Word16 ((:) * Double ((:) * Float ([] *))))))) t) => Tensor v1 t -> Tensor v2 t -> Tensor Value t
matTranspose :: TensorType a => Tensor v a -> Tensor Value a

-- | Computes the mean of elements across dimensions of a tensor.
--   
--   Reduces <tt>input</tt> along the dimensions given in
--   <tt>reduction_indices</tt>. Unless <tt>keep_dims</tt> is true, the
--   rank of the tensor is reduced by 1 for each entry in
--   <tt>reduction_indices</tt>. If <tt>keep_dims</tt> is true, the reduced
--   dimensions are retained with length 1.
mean :: (TensorType t, OneOf ((:) * (Complex Double) ((:) * (Complex Float) ((:) * Int16 ((:) * Int32 ((:) * Int64 ((:) * Int8 ((:) * Word16 ((:) * Word8 ((:) * Double ((:) * Float ([] *))))))))))) t, TensorType tidx, OneOf ((:) * Int32 ((:) * Int64 ([] *))) tidx) => Tensor v1 t -> Tensor v2 tidx -> Tensor Value t

-- | Returns x * y element-wise.
--   
--   <ul>
--   <li>NOTE*: <tt>Mul</tt> supports broadcasting. More about broadcasting
--   <a>here</a></li>
--   </ul>
mul :: (TensorType t, OneOf ((:) * (Complex Double) ((:) * (Complex Float) ((:) * Int16 ((:) * Int32 ((:) * Int64 ((:) * Int8 ((:) * Word16 ((:) * Word8 ((:) * Double ((:) * Float ([] *))))))))))) t) => Tensor v1 t -> Tensor v2 t -> Tensor Value t

-- | Computes numerical negative value element-wise.
--   
--   I.e., \(y = -x\).
neg :: (TensorType t, OneOf ((:) * (Complex Double) ((:) * (Complex Float) ((:) * Int32 ((:) * Int64 ((:) * Word16 ((:) * Double ((:) * Float ([] *)))))))) t) => Tensor v1 t -> Tensor Value t

-- | Packs a list of <tt>N</tt> rank-<tt>R</tt> tensors into one
--   rank-`(R+1)` tensor.
--   
--   Packs the <tt>N</tt> tensors in <tt>values</tt> into a tensor with
--   rank one higher than each tensor in <tt>values</tt>, by packing them
--   along the <tt>axis</tt> dimension. Given a list of tensors of shape
--   `(A, B, C)`;
--   
--   if `axis == 0` then the <tt>output</tt> tensor will have the shape
--   `(N, A, B, C)`. if `axis == 1` then the <tt>output</tt> tensor will
--   have the shape `(A, N, B, C)`. Etc.
--   
--   For example:
--   
--   ```prettyprint # <tt>x</tt> is [1, 4] # <tt>y</tt> is [2, 5] #
--   <tt>z</tt> is [3, 6] pack([x, y, z]) =&gt; [[1, 4], [2, 5], [3, 6]] #
--   Pack along first dim. pack([x, y, z], axis=1) =&gt; [[1, 2, 3], [4, 5,
--   6]] ```
--   
--   This is the opposite of <a>unpack</a>.
pack :: TensorType t => [Tensor v1 t] -> Tensor Value t
placeholder :: TensorType a => Shape -> Build (Tensor Value a)

-- | Creates a sequence of numbers.
--   
--   This operation creates a sequence of numbers that begins at
--   <tt>start</tt> and extends by increments of <tt>delta</tt> up to but
--   not including <tt>limit</tt>.
--   
--   For example:
--   
--   ``` # <tt>start</tt> is 3 # <tt>limit</tt> is 18 # <tt>delta</tt> is 3
--   tf.range(start, limit, delta) ==&gt; [3, 6, 9, 12, 15] ```
range :: (TensorType tidx, OneOf ((:) * Int32 ((:) * Int64 ((:) * Double ((:) * Float ([] *))))) tidx) => Tensor v1 tidx -> Tensor v2 tidx -> Tensor v3 tidx -> Tensor Value tidx

-- | Helper function for reduction ops (translation of
--   math_ops.reduced_shape).
reducedShape :: (OneOf '[Int32, Int64] t1, OneOf '[Int32, Int64] t2) => Tensor v1 t1 -> Tensor v2 t2 -> Tensor Value Int32

-- | Computes rectified linear: `max(features, 0)`.
relu :: (TensorType t, OneOf ((:) * Int16 ((:) * Int32 ((:) * Int64 ((:) * Int8 ((:) * Word16 ((:) * Word8 ((:) * Double ((:) * Float ([] *))))))))) t) => Tensor v1 t -> Tensor Value t

-- | Computes rectified linear gradients for a Relu operation.
reluGrad :: (TensorType t, OneOf ((:) * Int16 ((:) * Int32 ((:) * Int64 ((:) * Int8 ((:) * Word16 ((:) * Word8 ((:) * Double ((:) * Float ([] *))))))))) t) => Tensor v1 t -> Tensor v2 t -> Tensor Value t

-- | Reshapes a tensor.
--   
--   Given <tt>tensor</tt>, this operation returns a tensor that has the
--   same values as <tt>tensor</tt> with shape <a>shape</a>.
--   
--   If one component of <a>shape</a> is the special value -1, the size of
--   that dimension is computed so that the total size remains constant. In
--   particular, a <a>shape</a> of `[-1]` flattens into 1-D. At most one
--   component of <a>shape</a> can be -1.
--   
--   If <a>shape</a> is 1-D or higher, then the operation returns a tensor
--   with shape <a>shape</a> filled with the values of <tt>tensor</tt>. In
--   this case, the number of elements implied by <a>shape</a> must be the
--   same as the number of elements in <tt>tensor</tt>.
--   
--   For example:
--   
--   ```prettyprint # tensor <tt>t</tt> is [1, 2, 3, 4, 5, 6, 7, 8, 9] #
--   tensor <tt>t</tt> has shape [9] reshape(t, [3, 3]) ==&gt; [[1, 2, 3],
--   [4, 5, 6], [7, 8, 9]]
--   
--   # tensor <tt>t</tt> is [[[1, 1], [2, 2]], # [[3, 3], [4, 4]]] # tensor
--   <tt>t</tt> has shape [2, 2, 2] reshape(t, [2, 4]) ==&gt; [[1, 1, 2,
--   2], [3, 3, 4, 4]]
--   
--   # tensor <tt>t</tt> is [[[1, 1, 1], # [2, 2, 2]], # [[3, 3, 3], # [4,
--   4, 4]], # [[5, 5, 5], # [6, 6, 6]]] # tensor <tt>t</tt> has shape [3,
--   2, 3] # pass '[-1]' to flatten <tt>t</tt> reshape(t, [-1]) ==&gt; [1,
--   1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6]
--   
--   # -1 can also be used to infer the shape
--   
--   # -1 is inferred to be 9: reshape(t, [2, -1]) ==&gt; [[1, 1, 1, 2, 2,
--   2, 3, 3, 3], [4, 4, 4, 5, 5, 5, 6, 6, 6]] # -1 is inferred to be 2:
--   reshape(t, [-1, 9]) ==&gt; [[1, 1, 1, 2, 2, 2, 3, 3, 3], [4, 4, 4, 5,
--   5, 5, 6, 6, 6]] # -1 is inferred to be 3: reshape(t, [ 2, -1, 3])
--   ==&gt; [[[1, 1, 1], [2, 2, 2], [3, 3, 3]], [[4, 4, 4], [5, 5, 5], [6,
--   6, 6]]]
--   
--   # tensor <tt>t</tt> is [7] # shape `[]` reshapes to a scalar
--   reshape(t, []) ==&gt; 7 ```
reshape :: (TensorType t, TensorType tshape, OneOf ((:) * Int32 ((:) * Int64 ([] *))) tshape) => Tensor v1 t -> Tensor v2 tshape -> Tensor Value t

-- | Restore a tensor's value from a checkpoint file.
restore :: TensorType a => ByteString -> Tensor Ref a -> Build ControlNode

-- | Restore a tensor's value from a checkpoint file.
--   
--   This version allows restoring from a checkpoint file that uses a
--   different tensor name than the variable.
restoreFromName :: TensorType a => ByteString -> ByteString -> Tensor Ref a -> Build ControlNode
save :: TensorType a => ByteString -> [Tensor v a] -> Build ControlNode

-- | Create a constant scalar.
scalar :: TensorType a => a -> Tensor Value a
shape :: (TensorType t) => Tensor v1 t -> Tensor Value Int32

-- | Returns an element-wise indication of the sign of a number.
--   
--   `y = sign(x) = -1` if `x <a>0 if `x == 0`; 1 if `x</a> 0`.
--   
--   For complex numbers, `y = sign(x) = x / |x|` if `x != 0`, otherwise `y
--   = 0`.
sign :: (TensorType t, OneOf ((:) * (Complex Double) ((:) * (Complex Float) ((:) * Int32 ((:) * Int64 ((:) * Word16 ((:) * Double ((:) * Float ([] *)))))))) t) => Tensor v1 t -> Tensor Value t

-- | Returns the size of a tensor.
--   
--   This operation returns an integer representing the number of elements
--   in <tt>input</tt>.
--   
--   For example:
--   
--   ```prettyprint # <tt>t</tt> is [[[1, 1,, 1], [2, 2, 2]], [[3, 3, 3],
--   [4, 4, 4]]]] size(t) ==&gt; 12 ```
size :: (TensorType t, TensorType out_type, OneOf ((:) * Int32 ((:) * Int64 ([] *))) out_type) => Tensor v1 t -> Tensor Value out_type

-- | Computes softmax activations.
--   
--   For each batch <tt>i</tt> and class <tt>j</tt> we have
--   
--   softmax[i, j] = exp(logits[i, j]) / sum_j(exp(logits[i, j]))
softmax :: (TensorType t, OneOf ((:) * Word16 ((:) * Double ((:) * Float ([] *)))) t) => Tensor v1 t -> Tensor Value t

-- | Computes softmax cross entropy cost and gradients to backpropagate.
--   
--   Inputs are the logits, not probabilities.
softmaxCrossEntropyWithLogits :: (TensorType t, OneOf ((:) * Word16 ((:) * Double ((:) * Float ([] *)))) t) => Tensor v1 t -> Tensor v2 t -> (Tensor Value t, Tensor Value t)

-- | Converts a sparse representation into a dense tensor.
--   
--   Builds an array <tt>dense</tt> with shape <tt>output_shape</tt> such
--   that
--   
--   ```prettyprint # If sparse_indices is scalar dense[i] = (i ==
--   sparse_indices ? sparse_values : default_value)
--   
--   # If sparse_indices is a vector, then for each i
--   dense[sparse_indices[i]] = sparse_values[i]
--   
--   # If sparse_indices is an n by d matrix, then for each i in [0, n)
--   dense[sparse_indices[i][0], ..., sparse_indices[i][d-1]] =
--   sparse_values[i] ```
--   
--   All other values in <tt>dense</tt> are set to <tt>default_value</tt>.
--   If <tt>sparse_values</tt> is a scalar, all sparse indices are set to
--   this single value.
--   
--   Indices should be sorted in lexicographic order, and indices must not
--   contain any repeats. If <tt>validate_indices</tt> is true, these
--   properties are checked during execution.
sparseToDense :: (TensorType t, TensorType tindices, OneOf ((:) * Int32 ((:) * Int64 ([] *))) tindices) => Tensor v1 tindices -> Tensor v2 tindices -> Tensor v3 t -> Tensor v4 t -> Tensor Value t

-- | Returns x - y element-wise.
--   
--   <ul>
--   <li>NOTE*: <tt>Sub</tt> supports broadcasting. More about broadcasting
--   <a>here</a></li>
--   </ul>
sub :: (TensorType t, OneOf ((:) * (Complex Double) ((:) * (Complex Float) ((:) * Int32 ((:) * Int64 ((:) * Word16 ((:) * Double ((:) * Float ([] *)))))))) t) => Tensor v1 t -> Tensor v2 t -> Tensor Value t

-- | Computes the sum of elements across dimensions of a tensor.
--   
--   Reduces <tt>input</tt> along the dimensions given in
--   <tt>reduction_indices</tt>. Unless <tt>keep_dims</tt> is true, the
--   rank of the tensor is reduced by 1 for each entry in
--   <tt>reduction_indices</tt>. If <tt>keep_dims</tt> is true, the reduced
--   dimensions are retained with length 1.
sum :: (TensorType t, OneOf ((:) * (Complex Double) ((:) * (Complex Float) ((:) * Int16 ((:) * Int32 ((:) * Int64 ((:) * Int8 ((:) * Word16 ((:) * Word8 ((:) * Double ((:) * Float ([] *))))))))))) t, TensorType tidx, OneOf ((:) * Int32 ((:) * Int64 ([] *))) tidx) => Tensor v1 t -> Tensor v2 tidx -> Tensor Value t

-- | Shuffle dimensions of x according to a permutation.
--   
--   The output <tt>y</tt> has the same rank as <tt>x</tt>. The shapes of
--   <tt>x</tt> and <tt>y</tt> satisfy: `y.shape[i] == x.shape[perm[i]] for
--   i in [0, 1, ..., rank(x) - 1]`
transpose :: (TensorType t, TensorType tperm, OneOf ((:) * Int32 ((:) * Int64 ([] *))) tperm) => Tensor v1 t -> Tensor v2 tperm -> Tensor Value t
truncatedNormal :: TensorType a => Tensor v Int64 -> Build (Tensor Value a)

-- | Holds state in the form of a tensor that persists across steps.
--   
--   Outputs a ref to the tensor state so it may be read or modified.
--   TODO(zhifengc/mrry): Adds a pointer to a more detail document about
--   sharing states in tensorflow.
variable :: TensorType dtype => Shape -> Build (Tensor Ref dtype)

-- | Create a constant vector.
vector :: TensorType a => [a] -> Tensor Value a
zeros :: (Num a, TensorType a) => Shape -> Tensor Value a

-- | Returns a tensor of zeros with the same shape and type as x.
zerosLike :: TensorType t => Tensor v1 t -> Tensor Value t

-- | Reshape a N-D tensor down to a scalar.
--   
--   See <a>reshape</a>.
scalarize :: (TensorType a) => Tensor v a -> Tensor Value a
instance (TensorFlow.Types.TensorType a, GHC.Num.Num a, v ~ TensorFlow.Tensor.Value, TensorFlow.Types.OneOf '[GHC.Types.Double, GHC.Types.Float, GHC.Int.Int32, GHC.Int.Int64, Data.Complex.Complex GHC.Types.Float, Data.Complex.Complex GHC.Types.Double] a) => GHC.Num.Num (TensorFlow.Tensor.Tensor v a)


-- | Parallel lookups on the list of tensors.
module TensorFlow.EmbeddingOps

-- | Looks up <tt>ids</tt> in a list of embedding tensors.
--   
--   This function is used to perform parallel lookups on the list of
--   tensors in <tt>params</tt>. It is a generalization of <a>gather</a>,
--   where <tt>params</tt> is interpreted as a partition of a larger
--   embedding tensor.
--   
--   The partition_strategy is "mod", we assign each id to partition `p =
--   id % len(params)`. For instance, 13 ids are split across 5 partitions
--   as: `[[0, 5, 10], [1, 6, 11], [2, 7, 12], [3, 8], [4, 9]]`
--   
--   The results of the lookup are concatenated into a dense tensor. The
--   returned tensor has shape `shape(ids) + shape(params)[1:]`.
embeddingLookup :: (TensorType a, OneOf '[Int64, Int32] b, Num b) => [Tensor v a] -> Tensor Value b -> Build (Tensor Value a)

module TensorFlow.Gradient

-- | Gradient of <tt>y</tt> w.r.t. each element of <tt>xs</tt>.
gradients :: (Num (Tensor v1 a), v1 ~ Value, GradientCompatible a) => Tensor v1 a -> [Tensor v2 a] -> Build [Tensor Value a]
